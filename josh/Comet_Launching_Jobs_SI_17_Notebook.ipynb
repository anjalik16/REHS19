{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet: System Characteristics\n",
    "\n",
    "- Total peak flops ~2.1 PF\n",
    "- Dell primary integrator\n",
    "    - Intel Haswell processors w/ AVX2\n",
    "    - Mellanox FDR InfiniBand\n",
    "- 1944 standard compute nodes (46656 cores)\n",
    "    - Dual CPUs, each 12-core, 2.5 GHz\n",
    "    - 128 GB DDR4 2133 MHz DRAM\n",
    "    - 2\\*160GB GB SSDs (local disk)\n",
    "- 72 GPU nodes\n",
    "    - 36 nodes same as standard nodes *plus* two NVIDIA K80 cards, each with dual Kepler3 GPUs\n",
    "    - 36 nodes with 2 14-core Intel Broadwell CPUs plus 4 NVIDIA P100 GPUs\n",
    "- 4 large-memory nodes\n",
    "    - 1.5 TB DDR4 1866 MHz DRAM\n",
    "    - 4 Haswell processors/node\n",
    "    - 64 cores/node\n",
    "- Hybrid fat-tree topology\n",
    "    - FDR (56 Gbps) InfiniBand\n",
    "    - Rack-level (72 nodes, 1728 cores) full bisection bandwidth\n",
    "    - 4:1 oversubscription cross-rack\n",
    "- Performance Storage (Aeon)\n",
    "    - 7.6 PB, 200 GB/s; Lustre\n",
    "    - Scratch and Persistant Storage segments\n",
    "- Durable Storage (Aeon)\n",
    "    - 6 PB, 100 GB/s; Lustre\n",
    "    - Automatic backups of critical data\n",
    "- Home directory storage\n",
    "- Gateway hosting nodes\n",
    "- Virtual image repository\n",
    "- 100 Gbps external connectivity to Internet2 and ESNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet Network Architecture\n",
    "## InfiniBand Compute, Ethernet Storage\n",
    "\n",
    "![alt text](supercomputerarchitecture.png \"Supercomputer Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "\n",
    "## System Access - Logging in\n",
    "\n",
    "- Linux/Mac - Use available ssh clients\n",
    "- ssh clients for Windows - Putty, Cygwin\n",
    "    - http://www.chiark.greenend.org.uk/~sgtatham/putty/\n",
    "- Login hosts for the SDSC Comet:\n",
    "    - comet.sdsc.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging into Comet\n",
    "\n",
    "- Mac/Linux\n",
    "    - ssh username@comet.sdsc.edu\n",
    "- Windows (PuTTY)\n",
    "    - Host Name is comet.sdsc.edu\n",
    "    \n",
    "![alt text](loggingontocomet.png \"Logging onto Comet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet: Filesystems\n",
    "\n",
    "- Lusture filesystems - Good for scalable large block I/O\n",
    "    - Accessible from all compute and GPU nodes\n",
    "    - /oasis/scratch/comet - 2.5PB, peak performance: 100GB/s. Good location for storing large scale scratch data during a job\n",
    "    - /oasis/projects/nsf - 2.5PB, peak performance: 100GB/s. Long term storage\n",
    "    - **Not good for lots of small files or small block I/O**\n",
    "- SSD filesystems\n",
    "    - /scratch local to each native compute node - 210GB on regular compute nodes, 285GB on GPU, large memory nodes, 1.4TB on selected compute nodes\n",
    "    - SSD location is good for writing small files and temporary scratch files. Purged at the end of a job\n",
    "- Home directories (/home/#USER)\n",
    "    - Source trees, binaries, and small input files\n",
    "    - **Not good for large scale I/O**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet: System Environment\n",
    "\n",
    "- Modules used to manage environment for users\n",
    "- Default environment:\n",
    "    - \\$ module li\n",
    "    - Currently Loaded Modulefiles:\n",
    "        - intel/2013\\_sp1.2.144\n",
    "        - mvapich2\\_ib/2.1\n",
    "        - gnutools/2.69\n",
    "- Listing available modules:\n",
    "    - \\$ module av\n",
    "    - --------------------------/opt/modulefiles/mpi/.intel--------------------------\n",
    "    - intelmpi/2016.3.210(default) mvapich2\\_ib/2.1(default)\n",
    "    - mvapich2\\_gdr/2.1(default) openmpi\\_ib/1.8.4(default)\n",
    "    - mvapich2\\_gdr/2.2\n",
    "    - ------------------------/opt/modulefiles/applications/.intel-----------------\n",
    "    - ...\n",
    "    - ...\n",
    "- Loading modules:\n",
    "    - \\$ module load fftw/3.3.4\n",
    "    - \\$module li\n",
    "    - Currently Loaded Modulefiles:\n",
    "        - intel/2013\\_sp1.2.144\n",
    "        - mvapich2\\_ib/2.1\n",
    "        - gnutools/2.69\n",
    "        - fftw/3.3.4\n",
    "- See what a module does:\n",
    "    - \\$ module show fftw/3.3.4\n",
    "    - .----------------------------------------------------------------\n",
    "    - /optmodulefiles/applications/.intel/fftw/3.3.4:\n",
    "    - module-whatis fftw\n",
    "    - module-whatis Version: 3.3.4\n",
    "    - ...\n",
    "    - ...\n",
    "- \\\\$ echo \\$PATH\n",
    "    - /opt/fftw/3.3.4/intel/mvapich2\\_ib/bin:/share/apps/compute...\n",
    "- \\\\$ echo \\$FFTWHOME\n",
    "    - /opt/fftw/3.3.4/intel/mvapich2\\_ib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Programming\n",
    "\n",
    "- Comet supports MPI, OpenMP, and Pthreads for parallel programming. Hybrid modes are possible\n",
    "- GPU nodes support CUDA, Open ACC\n",
    "- MPI\n",
    "    - Default: mvapich2_ib/2.1\n",
    "    - Other options: openmpi_ib/1.8.4 (and 1.10.2), Intel MPI\n",
    "    - mvapich2_gdr: GPU direct enabled version\n",
    "- OpenMP: All compilers (GNU, Intel, PGI) have Open MP flags\n",
    "- Default Intel Compiler: intel/2013_sp1.2.144; *Versions 2015.2.164 and 2016.3.210 available*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Jobs on Comet\n",
    "\n",
    "- Important Note: **Do not run on the login nodes - even for simple tests**\n",
    "- All runs must be via the Slurm scheduling infrastructure\n",
    "    - Interactive Jobs: Use **srun** command:\n",
    "        - srun --pty --nodes=1 --ntasks-per-node=24 -p debug -t 00:30:00 --wait 0 /bin/bash\n",
    "    - Batch Jobs: Submit batch scripts from the login nodes. Can choose:\n",
    "        - Partition (details on upcoming cell)\n",
    "        - Time limit for the run (maximum of 48 hours)\n",
    "        - Number of nodes, tasks per node\n",
    "        - Memory requirements (if any)\n",
    "        - Job namd, output file location\n",
    "        - Email info, configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slurm Commands\n",
    "\n",
    "![alt text](slurmtable.png \"Slurm Table\")\n",
    "\n",
    "- Specified using -p option in batch script. For example:\n",
    "    - #SBATCH -p gpu\n",
    "- Submit jobs using the **sbatch** command:\n",
    "    - \\$ sbatch Localscratch-slurm.sb\n",
    "    - Submitted batch job 8718049\n",
    "- Check job status using the **squeue** command:\n",
    "    - \\\\$ squeue -u \\$USER\n",
    "    - JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n",
    "    - 8718049 compute localscr mahidhar PD 0:00 1 (Priority)\n",
    "- Once the job is running:\n",
    "    - \\\\$ squeue -u \\$USER\n",
    "    - JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON)\n",
    "    - 8718064 debug localscr mahidhar R 0:02 1 comet-14-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet Compute Nodes\n",
    "## 2-Socket (Total 24 cores) Intel Haswell Processors\n",
    "\n",
    "- Hands on Examples Using:\n",
    "    - MPI\n",
    "    - OpenMP\n",
    "    - HYBRID\n",
    "    - Local scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet - Compiling/Running Jobs\n",
    "\n",
    "- Copy and change to directory (assuming you already copied the PHYS244 directory):\n",
    "    - cd /home/\\$USER/SI2017/MPI\n",
    "- Verify modules loaded:\n",
    "    - module list\n",
    "    - Currently Loaded Modulefiles:\n",
    "        - intel/2013\\_sp1.2.144\n",
    "        - mvapich2\\_ib/2.1\n",
    "        - gnutools/2.69\n",
    "- Compile the MPI hello world code:\n",
    "    - mpif90 -o hello\\_mpi hello\\_mpi.f90\n",
    "- Verify executable has been created:\n",
    "    - Is -It hello_mpi\n",
    "    - -rwxr-xr-x 1 mahidhar sdsc 721912 Mar 25 14:53 hello_mpi\n",
    "- Submit job from IBRUN directory:\n",
    "    - cd /home/\\$USER/SI2017/MPI/IBRUN\n",
    "    - sbatch --res=SI2017DAY1 hellompi-slurm.sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet - Hello World on Compute Nodes\n",
    "\n",
    "The submit script is hellompi-slurm.sb:\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=\"hellompi\"\n",
    "#SBATCH --output=\"hellompi.%j.%N.out\"\n",
    "...\n",
    "...\n",
    "\n",
    "#This job runs with 2 nodes, 24 cores per node for a total fo 48 cores\n",
    "#ibrun in verbose mode will give binding detail\n",
    "\n",
    "ibrun -v ./hello_mpi\n",
    "IBRUN: Command is ../hello_mpi\n",
    "IBRUN: Command is /share/apps/examples/MPI/hello_mpi\n",
    "...\n",
    "...\n",
    "node      18 : Hello world\n",
    "node      13 : Hello world\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling OpenMP Example\n",
    "\n",
    "- Change to the examples directory:\n",
    "    - cd /home/\\$USER/SI2017/OPENMP\n",
    "- Compile using -openmp flag:\n",
    "    - ifort -o hello\\_openmp -openmp hello\\_openmp.f90\n",
    "- Verify executable was created:\n",
    "    - [mahidhar@comet-08-11 OPENMP]\\$ Is -It hello\\_openmp\n",
    "    - -rwxr-xr-x 1 mahidhar sdsc 750648 Mar 25 15:00 hello_openmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenMP job script\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=\"hell\\_openmp\"\n",
    "#SBATCH --output=\"hello\\_openmp.%j.%N.out\"\n",
    "...\n",
    "...\n",
    "\n",
    "#SET the number of openmp threads\n",
    "export OMP_NUM_THREADS=24\n",
    "\n",
    "#Run the job using mpirun_rsh\n",
    "./hello_openmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output from the OpenMP Job\n",
    "\n",
    "\\$ more hello_openmp.out\n",
    "HELLO FROM THREAD NUMBER = 7\n",
    "HELLO FROM THREAD NUMBER = 6\n",
    "HELLO FROM THREAD NUMBER = 9\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Hybrid (MPI + OpenMP) Jobs\n",
    "\n",
    "Several HPC codes use a hbrid MPI, OpenMP approach\n",
    "\n",
    "\"ibrun\" wrapper developed to handle such hybrid use cases. Automatically senses the MPI build (mvapich2, openmpi) and binds tasks correctly\n",
    "\n",
    "\"ibrun -help\" gives detailed usage info\n",
    "\n",
    "hello\\_hybrid.c is a sample code, and hello_hybrid.cmd shows \"ibrun\" usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hello_hybrid.cmd\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=\"hellohybrid\"\n",
    "#SBATCH --output=\"hellohybrid.%j.%N.out\"\n",
    "...\n",
    "...\n",
    "\n",
    "export OMP_NUM_THREADS=6\n",
    "ibrun --npernode 4 ./hello_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Code Output\n",
    "\n",
    "\\[etrain61@comet-ln3 HYBRID]$ more hellohybrid.8557716.comet-14-01.out\n",
    "\n",
    "Hello from thread 0 out of 6 from process 2 out of 8 on comet-14-01.local\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SDD Scratch\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=\"localscratch\"\n",
    "\n",
    "...\n",
    "\n",
    "#Copy binary to SSD\n",
    "\n",
    "cp IOR.exe /scratch/\\\\$USER/\\$SLURM_JOBID\n",
    "\n",
    "#Change to local scratch (SSD) and run IOR benchmark\n",
    "\n",
    "cd /scratch/\\\\$USER/\\\\$SLURM_JOBID\n",
    "\n",
    "#Run IO benchmark\n",
    "\n",
    "ibrun -np 4 ./IOR.exe -F -t 1m -b 4g -v --v > IOR.out.\\\\$SLURM\\\\_JOBID\n",
    "\n",
    "#Copy out data you need\n",
    "\n",
    "cp IOR.out.\\\\$SLURM_JOBID $SLURM_SUBMIT_DIR\n",
    "\n",
    "- Snapshot on the node during the run:\n",
    "\n",
    "\\$ pwd\n",
    "\n",
    "/scratch/mahidhar/435463\n",
    "\n",
    "\\$ ls -It\n",
    "\n",
    "total 22548292\n",
    "\n",
    "-rw-r-- 1 mahidhar hpss 5429526528 May 15 23:48 testFile.00000001\n",
    "\n",
    "...\n",
    "\n",
    "- Performance from single node (in log file copied back):\n",
    "    - Max Write: 250.52 MiB/sec (262.69 MB/sec)\n",
    "    - Max Read: 181.92 MiB/sec (190.76 MB/sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comet GPU Nodes\n",
    "## 2 NVIDIA K-80 Cards (4 GPUs total) per node\n",
    "\n",
    "[1] CUDA code compiles and run example\n",
    "\n",
    "[2] Hands on Examples using Sungularity to enable Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling CUDA Example\n",
    "\n",
    "- Load the CUDA module:\n",
    "    - module load cuda\n",
    "- Compile the code:\n",
    "    - cd /home/\\$USER/SI2017/CUDA\n",
    "    - nvcc -o matmul -l. matrixMul.cu\n",
    "- Submit the job:\n",
    "    - sbatch --res=SI2017DAY1 cuda.sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Example: Batch Submission Script\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=\"CUDA\"\n",
    "#SBATCH --output=\"CUDA.%j.%N.out\"\n",
    "\n",
    "...\n",
    "\n",
    "#Load the cuda module\n",
    "\n",
    "module load cuda\n",
    "\n",
    "#Run the job\n",
    "\n",
    "./matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sungularity: Provides Flexibility for OS Environment\n",
    "\n",
    "- Singulatiry (http://singularity.lbl.gov) is a relatively new development that has become very popular on Comet\n",
    "- Singularity allows groups to easily migrate complex software stacks from their campus to Comet\n",
    "- Singularity runs in user space, and requires very special support - in fact it actually reduces it in some cases\n",
    "- We have roughly 15 groups running this on Comet\n",
    "- Applications include: Tensorflow, Paraview, Torch, Fenics, and custom user applications\n",
    "- Docker images can be imported into Singularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow via Singularity\n",
    "\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name\"TensorFlow\"\n",
    "#SBATCH --output=\"TensorFlow.%j.%N.out\"\n",
    "#SBATCH --partition=gpu-shared\n",
    "...\n",
    "#SBATCH --gres=gpu:k80:1\n",
    "\n",
    "#Run the job\n",
    "\n",
    "module load singularity\n",
    "\n",
    "singularity exec /share/apps/gpu/singularity/sdsc_ubuntu_gpu_tflow.img lsb_release -a\n",
    "\n",
    "singularity exec /share/apps/gpu/singularity/sdsc_ubuntu_gpu_tflow.img python -m tensorflow.models.image.mnist.convolutional\n",
    "\n",
    "- Change to the examples directory:\n",
    "cd /home/$USER/SI2017/TensorFlow\n",
    "\n",
    "- Submit the job:\n",
    "sbatch --res=SI2017DAY1 TensorFlow.sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Example: Output\n",
    "\n",
    "Distributor ID: Ubuntu\n",
    "\n",
    "Description: Ubuntu 16.04 LTS\n",
    "\n",
    "Release: 16.04\n",
    "\n",
    "Codename: xenial\n",
    "\n",
    "I tensorflow/stream_executor/dso_loader.cc:108] successrully opened CUDA library libcublas.so locally\n",
    "\n",
    "...\n",
    "\n",
    "I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0)->(device: 0, name: Tesla K80, pci bus id: 0000:85:00.0)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Data Analysis to Existing Compute Infrastructure\n",
    "\n",
    "![alt text](structure1.png \"Structure 1\")\n",
    "\n",
    "![alt text](structure2.png \"Structure 2\")\n",
    "\n",
    "![alt text](structure3.png \"Structure 3\")\n",
    "\n",
    "![alt text](structure4.png \"Structure 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANAGRAM Example\n",
    "\n",
    "- Change to directory:\n",
    "\n",
    "cd $HOME/SI2017/hadoop/ANAGRAM_Hadoop2\n",
    "\n",
    "- Submit job:\n",
    "\n",
    "sbatch --res=SI2017DAY1 anagram.script\n",
    "\n",
    "- Check configuration in directory:\n",
    "\n",
    "ls $HOME/cometcluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anagram Example - Sample Output\n",
    "\n",
    "cat part-00000\n",
    "\n",
    "...\n",
    "\n",
    "aabcdelmnu manducable, ambulanced,\n",
    "\n",
    "aabcdeorrsst broadcasters, rebroadcasts,\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDMA-Hadoop and RDMA-Spark\n",
    "## Network-Based Computing Lab, Ohio State University\n",
    "### NSF funded project in collaboration with DR. DK Panda\n",
    "\n",
    "- HDFS, MapReduce, and RPC over native InfiniBand and RDMA over Converged Ethernet (RoCE)\n",
    "- Based on Apache distributions of Hadoop and Spark\n",
    "- Version RDMA-Apache-Hadoop-2.x 1.1.0 (based on Apache Hadoop 2.6.0) available on Comet\n",
    "- Version RDMA-Spark 0.9.3 (based on Apache Spark 1.5.1) is availaable on Comet\n",
    "- More details on the RDMA-Hadoop and RDMA-Spark projects at:\n",
    "    - http://hibd.cse.ohio-state.edu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDMA-Hadoop, Spark\n",
    "\n",
    "- Exploit performance on modern clusters with RDMA-enabled interconnects for Big Data applications\n",
    "- Hybrid design with in-memory and hetergeneous storage (HDD, SSDs, Lustre)\n",
    "- Keep compliance with standard distributions from Apache\n",
    "\n",
    "![alt text](RDMA.png \"RDMA-Hadoop Spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands On: Anagram using HHH-M mode\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=\"rdmahadoopanagram\"\n",
    "\n",
    "#SBATCH --output=\"rdmahadoopanagram.%j.%N.out\"\n",
    "\n",
    "...\n",
    "\n",
    "#Script request 3 nodes - one used for namenode, 2 for data nodes/processing\n",
    "\n",
    "#Set modulepath and load RDMA Hadoop Module\n",
    "\n",
    "export\n",
    "\n",
    "MODULEPATH=/share/apps/compute/modulefiles/applications:$MODULEPATH\n",
    "\n",
    "module load rdma-hadoop/2x-1.1.0\n",
    "\n",
    "#Get the host list\n",
    "\n",
    "export SLURM\\_NODEFILE=\\`generate_pbs_nodefile`\n",
    "\n",
    "cat #SLURM\\_NODEFILE | sort -u > hosts.hadoop.list\n",
    "\n",
    "#Use SLURM integrated configuration/startup script\n",
    "\n",
    "hibd_install_configure_start.sh -s -n ./hosts.hadoop.list -i \\\\$SLURM\\_JOBID -h \\\\$HADOOP\\_HOME -j \\\\$JAVA\\_HOME -m hhh-m -r /dev/shm -d /scratch/\\\\$USER/\\\\$SLURM\\_JOBID -t /scratch/\\\\$USER/$SLURM_JOBID/hadoop_local\n",
    "\n",
    "#Commands to run ANAGRAM example\n",
    "\n",
    "\\\\$HADOOP\\_HOME/bin/hdfs --config \\\\$HOME/conf_\\\\$SLURM_JOBID dfs -mkdir -p /user/$USER/input\n",
    "\n",
    "\\\\$HADOOP\\_HOME/bin/hdfs --config \\\\$HOME/conf_\\\\$SLURM_JOBID dfs -put SINGLE.TXT /user/$USER/input/SINGLE.TXT\n",
    "\n",
    "\\\\$HADOOP\\_HOME/bin/hadoop --config \\\\$HOME/conf_\\\\$SLURM_JOBID jar AnagramJob.jar /user/\\\\$USER/input/SINGLE.TXT /user/\\$HOME/output\n",
    "\n",
    "\\\\$HADOOP\\_HOME/bin/hdfs --config \\\\$HOME/conf_\\\\$SLURM_JOBID dfs -get /user/\\\\$USER/output/part* $SLURM_WORKING_DIR\n",
    "\n",
    "#Clean up\n",
    "\n",
    "hibd_stop_cleanup.sh -d -h $HADOOP_HOME -m hhh-m -r /dev/shm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDMA-Hadoop: HHH-M Example\n",
    "\n",
    "- Change to directory:\n",
    "\n",
    "cd $HOME/SI2017/hadoop/RDMA-Hadoop/RDMA-HHH-M\n",
    "\n",
    "- Submit job:\n",
    "\n",
    "sbatch --res=SI2017DAY1 anagram.script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "- Comet can be directly accessed using a ssh client\n",
    "- Always run via the batch scheduler - for both interactive and batch jobs. **Do not run on the login nodes**\n",
    "- Chooose your filesystem wisely - Lustre parallel filesystem for large block I/O. SSD based filesystems for small block I/O, lots of small files. **Do not use home filesystem for intensive I/O of any kind**\n",
    "- Comet can handle MPI, OpenMP, Pthreads, Hybrid, CUDA, and OpenACC jobs. Singularity provides further flexibility\n",
    "- Dynamic spin up of Hadoop, Spark instances within Comet scheduler framework"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
